{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04994f1-095a-4b20-a48a-05116c0ffab6",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecedcb-3d38-4fcf-abbd-1d2941f5b0d6",
   "metadata": {},
   "source": [
    "Ans - The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning method used for classification and regression. It's based on the idea that similar data points tend to belong to the same class or have similar values.\n",
    "\n",
    "When predicting a new data point's class or value, KNN finds the 'k' closest data points in the training set based on a distance metric, like Euclidean distance. For classification, it takes a majority vote among the neighbors. In regression, it averages their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb615d-5f9b-45b1-94f9-958631738175",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4d6e3-6d50-4875-9834-e5abb61f6a92",
   "metadata": {},
   "source": [
    "Ans - 1] Square Root of N: A simple rule of thumb is to start with 'K' as the square root of the number of samples in your dataset. It often provides a decent starting point.\n",
    "\n",
    "2] Odd vs. Even: In classification tasks, using an odd value for 'K' helps avoid ties in voting between classes.\n",
    "\n",
    "3] Cross-Validation: Split your dataset into multiple folds (e.g., 5-fold or 10-fold cross-validation). For each fold, train the KNN model with different 'K' values and evaluate its performance on the remaining fold. Choose the 'K' value that gives the best average performance across all folds.\n",
    "\n",
    "4] Grid Search: Define a range of 'K' values to try (e.g., 1 to 20). Combine this with a grid search over other hyperparameters (like distance metric) to find the best combination that minimizes the error rate on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312ef78-9689-4ae3-aa5c-07fe59519a5a",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022abac-77c3-4133-869c-2d6ae0db5366",
   "metadata": {},
   "source": [
    "Ans - 1] KNN Classifier:\n",
    "\n",
    "a. Task: Predicts the class label of a new data point.\n",
    "\n",
    "b. Output: Discrete categorical values (e.g., \"red,\" \"green,\" \"blue\" or \"spam,\" \"not spam\").\n",
    "\n",
    "c. Prediction Method: Finds the 'K' nearest neighbors of the new data point. Takes a majority vote among the neighbors' class labels. Assigns the most frequent class label as the prediction.   \n",
    "\n",
    "2] KNN Regressor:\n",
    "\n",
    "a. Task: Predicts a continuous numerical value for a new data point.\n",
    "\n",
    "b. Output: Continuous values (e.g., temperature, price, or stock value).\n",
    "\n",
    "c. Prediction Method: Finds the 'K' nearest neighbors of the new data point. Calculates the average (or sometimes weighted average) of the target values of those neighbors. Uses the calculated average as the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae830f39-3ba0-4f65-8bc7-baa91f3acaf1",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf32f5e-d6a8-4d1d-a32c-0edbd2865632",
   "metadata": {},
   "source": [
    "Ans - 1] For KNN Classification:\n",
    "\n",
    "a. Accuracy: This is the most common metric, representing the proportion of correctly classified instances. However, it can be misleading if classes are imbalanced.\n",
    "\n",
    "b. Confusion Matrix: A table summarizing the model's predictions versus the actual labels. It reveals details like true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "c. Precision, Recall, and F1 Score: These metrics provide a more nuanced view, especially for imbalanced datasets. Precision measures the accuracy of positive predictions, recall measures how well the model finds all positive instances, and the F1 score is the harmonic mean of precision and recall.\n",
    "\n",
    "2] For KNN Regression:\n",
    "\n",
    "a. Mean Squared Error (MSE): The average of the squared differences between predicted and actual values. Lower values indicate better fit.\n",
    "\n",
    "b. Root Mean Squared Error (RMSE): The square root of MSE, providing an error measure in the same units as the target variable.\n",
    "\n",
    "c. Mean Absolute Error (MAE): The average of the absolute differences between predicted and actual values. Less sensitive to outliers than MSE.\n",
    "\n",
    "d. R-squared (R²): Represents the proportion of the variance in the target variable explained by the model. Higher values (closer to 1) indicate better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b583a-f255-4f6c-ba1b-99b9c3b60bd7",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79117d-3a44-422d-9088-374f4b6280c7",
   "metadata": {},
   "source": [
    "Ans - The curse of dimensionality poses significant challenges for KNN in high-dimensional data. As the number of features increases, data points become sparse, making it difficult to find meaningful neighbors. This sparsity also leads to increased computational costs, as calculating distances becomes more complex. Irrelevant features can further hinder KNN's performance by introducing noise and misleading the algorithm. Additionally, overfitting becomes a greater risk due to the increased likelihood of finding neighbors that are not truly representative of the data's underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9c1f7-1f3c-4f2a-a1ac-b4b76ba2c637",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fe6c8-01c8-4e3d-8c05-c9c76d21d72e",
   "metadata": {},
   "source": [
    "To handle missing values in KNN, you can either impute them (replace with estimated values), delete them (remove rows or exclude from calculations), or use feature engineering techniques like creating indicators. The best approach depends on the amount of missing data, the mechanism of missingness, and the dataset size. Simple methods like mean imputation or listwise deletion might suffice for small amounts of missing data, while more advanced techniques like multiple imputation are suitable for larger proportions or non-random missingness. Consider computational resources and experiment to find the best strategy for your specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62074e1c-2d85-43b4-847d-7cf1a48fe863",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6de852-31d7-4e13-ba4f-c69f810ec16c",
   "metadata": {},
   "source": [
    "Ans - 1] KNN Classifier\n",
    "\n",
    "a. Suitable for: Classification problems (categorical output)\n",
    "\n",
    "b. Evaluation Metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "\n",
    "c. Bias-Variance: High variance, low bias with small K\n",
    "\n",
    "d. Curse of Dimensionality: More susceptible\n",
    "\n",
    "2] KNN Regressor\n",
    "\n",
    "a. Suitable for: Regression problems (continuous output)\n",
    "\n",
    "b. Evaluation Metrics: MSE, RMSE, MAE, R-squared\n",
    "\n",
    "c. Bias-Variance: High bias, low variance with large K\n",
    "\n",
    "d. Curse of Dimensionality: Less affected\n",
    "\n",
    "3] Choosing the right model depends on:\n",
    "\n",
    "a. Output type: Categorical (classifier) or continuous (regressor)\n",
    "\n",
    "b. Problem complexity: Interpretability vs. accuracy\n",
    "\n",
    "c. Dataset size and dimensionality\n",
    "\n",
    "d. Data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afa321-62cd-41b0-819e-8e9575c70ac4",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce31f8-8066-4d16-a864-15d80508b0d9",
   "metadata": {},
   "source": [
    "Ans - 1] Strengths of KNN Classification:\n",
    "\n",
    "a. Simple and Intuitive: KNN is easy to understand and implement, making it a good choice for beginners in machine learning.\n",
    "\n",
    "b. Non-Parametric: It makes no assumptions about the underlying data distribution, making it flexible for various types of data.\n",
    "\n",
    "2] Strengths of KNN Regression:\n",
    "\n",
    "a. Non-Linear: Can model complex non-linear relationships between features and the target variable.\n",
    "\n",
    "b. Local Adaptation: Predictions are based on local neighborhoods, allowing it to capture local patterns in the data.\n",
    "\n",
    "3] Weaknesses of KNN Classification:\n",
    "\n",
    "a. Computational Cost: Can be computationally expensive for large datasets, as it needs to calculate distances to all training instances.\n",
    "\n",
    "b. Curse of Dimensionality: Performance degrades with high-dimensional data due to increased sparsity.\n",
    "\n",
    "c. Sensitive to Irrelevant Features: Performance can be negatively affected by irrelevant or noisy features.\n",
    "\n",
    "4] Weaknesses of KNN Regression:\n",
    "\n",
    "a. Choice of K: The value of K can significantly impact model performance and needs careful tuning.\n",
    "\n",
    "b. Scaling of Features: Requires feature scaling to ensure that all features contribute equally to distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ff31d-ee32-4d80-b14a-69f723dac9f9",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28b2fe-1cd7-49ec-899a-23cf4adb64bc",
   "metadata": {},
   "source": [
    "Ans - Euclidean and Manhattan distances are both used in KNN to measure how different data points are. Euclidean distance is like a straight line between two points, calculated with the Pythagorean theorem. It's good for continuous features on the same scale but can be sensitive to outliers. Manhattan distance, on the other hand, is like the distance a taxi travels on city blocks. It's calculated by adding the absolute differences between coordinates. It's better for discrete features or features with different scales, and less affected by outliers. Choosing the right distance depends on your data: use Euclidean for continuous features on the same scale and Manhattan for discrete or differently scaled features. In higher dimensions, Manhattan might be better as it's less sensitive to the curse of dimensionality. Consider outliers and the meaning of your features when making your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb8636-1a13-4815-962a-8f9c1ba10c0f",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3eaf1-dce3-48e1-a77e-3e9e35ad5e33",
   "metadata": {},
   "source": [
    "Feature scaling is crucial in KNN because it ensures all features contribute equally to distance calculations, regardless of their original scales. Without scaling, features with larger values dominate the distance, leading to inaccurate predictions. By bringing all features to a similar range, usually 0 to 1 or with a mean of 0 and standard deviation of 1, scaling prevents this bias. This improves the accuracy and generalization of the KNN model, ensuring no single feature disproportionately influences the results. Common scaling techniques include standardization and normalization, and it's generally recommended for KNN and other algorithms like SVM, linear regression, and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b1836-2a06-4482-8614-9c8bfaf55f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12c285-508b-4a55-9e6f-cea6ec00cf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
